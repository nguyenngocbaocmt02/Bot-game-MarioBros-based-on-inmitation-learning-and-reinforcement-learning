{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf7b60c",
   "metadata": {
    "id": "eaf7b60c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627a1e2d",
   "metadata": {
    "id": "627a1e2d"
   },
   "outputs": [],
   "source": [
    "init_size = 50\n",
    "pop_size = 50\n",
    "time_remainning = 400\n",
    "mutate1_prob = 0.05\n",
    "mutate2_prob = 0.02\n",
    "mutate3_prob = 0.05\n",
    "mutate4_prob = 0.05\n",
    "crossover_prob = 0.1\n",
    "status = {\"small\" : 0, \"tall\" : 1, \"fireball\" : 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IPkPPUlAC3dU",
   "metadata": {
    "id": "IPkPPUlAC3dU"
   },
   "source": [
    "# GA upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f8f15e",
   "metadata": {
    "id": "85f8f15e"
   },
   "outputs": [],
   "source": [
    "class Individual:\n",
    "    def __init__(self, actions=None, fitness =None, score = None, dis = None):\n",
    "        self.fitness = 0\n",
    "        self.score = 0\n",
    "        self.dis = 0\n",
    "        if actions == None:\n",
    "            actions = []\n",
    "            for i in range(init_size):\n",
    "                action = random.randrange(0, len(SIMPLE_MOVEMENT))\n",
    "                rep = random.randrange(1, 12)\n",
    "                for j in range(rep):\n",
    "                    actions.append(action)\n",
    "            self.actions = actions\n",
    "            self.evaluate()\n",
    "        else:\n",
    "            if fitness == None:\n",
    "                self.actions = actions\n",
    "                self.evaluate()\n",
    "            else:\n",
    "                self.actions = actions\n",
    "                self.fitness = fitness\n",
    "                self.score = score\n",
    "                self.dis = dis\n",
    "                \n",
    "    def evaluate(self):\n",
    "        fitness = 0\n",
    "        tmp_actions = []\n",
    "        cur_status = 0\n",
    "        reward = 0\n",
    "        done = 0 \n",
    "        info = 0\n",
    "        state = env.reset()\n",
    "        for action in self.actions:\n",
    "            for qq in range(stack_frame):\n",
    "                state, reward, done, info = env.step([action])\n",
    "                if info[0]['flag_get'] == True:\n",
    "                    tmp_actions.append(action)\n",
    "                    fitness += reward[0]\n",
    "                    self.dis = info[0]['x_pos']\n",
    "                    self.score = info[0]['score']\n",
    "                    self.actions = tmp_actions\n",
    "                    self.fitness = fitness + info[0]['score'] * 5\n",
    "                    return \n",
    "                if  'terminal_observation' in info[0] or status[info[0]['status']] < cur_status:\n",
    "                    for i in range(24):\n",
    "                        if len(tmp_actions) > 1:\n",
    "                            del tmp_actions[-1]\n",
    "                    self.actions = tmp_actions\n",
    "                    self.evaluate()\n",
    "                    return\n",
    "                cur_status = status[info[0]['status']]\n",
    "                fitness += reward[0]\n",
    "            tmp_actions.append(action)\n",
    "        self.dis = info[0]['x_pos'] \n",
    "        self.score = info[0]['score']\n",
    "        self.actions = tmp_actions\n",
    "        self.fitness = fitness + info[0]['score'] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71bafb8",
   "metadata": {
    "id": "b71bafb8"
   },
   "outputs": [],
   "source": [
    "class Population:\n",
    "    def __init__(self, save_file = \"\", gen = 0):\n",
    "        self.pop = []\n",
    "        self.gen = 0\n",
    "        if(save_file == \"\"):\n",
    "            for i in range(0, pop_size):\n",
    "                print(i)\n",
    "                self.pop.append(Individual())\n",
    "        else:\n",
    "            self.loadFileToPop(save_file, gen)\n",
    "\n",
    "    def evolve(self):\n",
    "        for ind in self.pop:\n",
    "            print(ind.fitness, ind.score, ind.dis)\n",
    "        print(\"--------------------------------------------\")\n",
    "        length = len(self.pop)\n",
    "        for i in range(0, length - pop_size):\n",
    "            del self.pop[-1]\n",
    "        a = random.randrange(0, int(pop_size / 10))\n",
    "        while True:\n",
    "            b = random.randrange(0, int(pop_size / 5))\n",
    "            if b != a:\n",
    "                break\n",
    "        self.pop[a] = self.mutate2(self.pop[a])\n",
    "        self.pop[a] = self.mutate3(self.pop[a])\n",
    "        self.pop[b] = self.mutate2(self.pop[b])\n",
    "        self.pop[b] = self.mutate3(self.pop[b])\n",
    "        for i, ind in enumerate(self.pop):\n",
    "            alpha = random.random()\n",
    "            if alpha <= mutate1_prob:\n",
    "                self.pop[i] = self.mutate1(self.pop[i])\n",
    "            elif alpha <= mutate1_prob + mutate2_prob:\n",
    "                self.pop[i] = self.mutate2(self.pop[i])\n",
    "            elif alpha <= mutate1_prob + mutate2_prob + mutate3_prob:\n",
    "                self.pop[i] = self.mutate3(self.pop[i])\n",
    "            elif alpha <= mutate1_prob + mutate2_prob + mutate3_prob + mutate4_prob:\n",
    "                self.pop[i] = self.mutate4(self.pop[i])    \n",
    "        for i in range(int(crossover_prob * pop_size)):\n",
    "            a = random.randrange(0, int(pop_size / 5))\n",
    "            while True:\n",
    "                b = random.randrange(0, pop_size)\n",
    "                if b != a:\n",
    "                    break\n",
    "            ind1, ind2 = self.crossOver(copy.deepcopy(self.pop[a]), copy.deepcopy(self.pop[b]))\n",
    "            self.pop.append(ind1)\n",
    "            self.pop.append(ind2)\n",
    "        self.pop.sort(key=lambda x: x.fitness, reverse=True)\n",
    "        self.gen += 1\n",
    "\n",
    "    def crossOver(self, ind1, ind2):\n",
    "        child1 = []\n",
    "        child2 = []\n",
    "        cut_ind = random.randrange(0, min(len(ind1.actions), len(ind2.actions)))\n",
    "        for i in range(0, cut_ind):\n",
    "            child1.append(ind1.actions[i])\n",
    "            child2.append(ind2.actions[i])\n",
    "        child1.append(int((ind1.actions[cut_ind] + ind2.actions[cut_ind]) / 2))\n",
    "        child2.append(int((ind1.actions[cut_ind] + ind2.actions[cut_ind]) / 2))\n",
    "        for i in range(cut_ind + 1, len(ind2.actions)):\n",
    "            child1.append(ind2.actions[i])\n",
    "        for i in range(cut_ind + 1, len(ind1.actions)):\n",
    "            child2.append(ind1.actions[i])\n",
    "        return Individual(child1), Individual(child2)\n",
    "\n",
    "    # chen vao giua chuoi hanh dong\n",
    "    def mutate1(self, ind):\n",
    "        t = copy.deepcopy(ind)\n",
    "        start = random.randrange(0, len(t.actions))\n",
    "        action = random.randrange(0, len(SIMPLE_MOVEMENT))\n",
    "        rep = random.randrange(1, 12)\n",
    "        actions = t.actions[:start] + [action for i in range(rep)] + t.actions[start:]\n",
    "        x = Individual(actions)\n",
    "        del t\n",
    "        if x.fitness > ind.fitness:\n",
    "            return x\n",
    "        else:\n",
    "            return ind\n",
    "\n",
    "    # chen vao cuoi chuoi hanh dong\n",
    "    def mutate2(self, ind):\n",
    "        t = copy.deepcopy(ind)\n",
    "        res = []\n",
    "        res2 = []\n",
    "        rep = random.randrange(1, 12)\n",
    "        for action in range(1, len(SIMPLE_MOVEMENT) - 1):\n",
    "            if(action == 2):\n",
    "                continue\n",
    "            if(action == 4 and (ind.actions[-1] == 4 or ind.actions[-1] == 2)):\n",
    "                actions = t.actions + [0] + [action for i in range(rep)]\n",
    "            else:\n",
    "                actions = t.actions + [action for i in range(rep)]\n",
    "            x = Individual(actions)\n",
    "            print(action, x.fitness)\n",
    "            if x.fitness > t.fitness:\n",
    "                res.append(x)\n",
    "            if x.fitness == t.fitness:\n",
    "                res2.append(x)\n",
    "        if len(res) == 0:\n",
    "            if(len(res2) == 0):\n",
    "                for i in range(24):\n",
    "                    if len(t.actions) > 1:\n",
    "                        del t.actions[-1]\n",
    "                t.evaluate()\n",
    "                return t\n",
    "            else:\n",
    "                return res2[random.randrange(0, len(res2))]\n",
    "        return res[random.randrange(0, len(res))]\n",
    "            \n",
    "    # chen nhay vao cuoi chuoi hanh dong\n",
    "    def mutate3(self, ind):\n",
    "        t = copy.deepcopy(ind)\n",
    "        rep = random.randrange(4, 12)\n",
    "        actions = t.actions + [0]\n",
    "        actions = actions + [4 for i in range(rep)]\n",
    "        x = Individual(actions)\n",
    "        del t\n",
    "        if x.fitness > ind.fitness:\n",
    "            return x\n",
    "        else:\n",
    "            return ind   \n",
    "            \n",
    "    # chen chuoi vao giua\n",
    "    def mutate4(self, ind):\n",
    "        t = copy.deepcopy(ind)\n",
    "        rep = random.randrange(1, 12)\n",
    "        action = random.randrange(0, len(SIMPLE_MOVEMENT))\n",
    "        start = random.randrange(0, len(t.actions))\n",
    "        actions = t.actions\n",
    "        for i in range(start, min(len(actions), start + rep)):\n",
    "            actions[i] = action\n",
    "        x = Individual(actions)\n",
    "        if x.fitness > ind.fitness:\n",
    "            return x\n",
    "        else:\n",
    "            return ind\n",
    "    \n",
    "    def savePopToFile(self, save_file):\n",
    "        save = open(save_file, 'w')\n",
    "        for ind in self.pop:\n",
    "            for j in ind.actions:\n",
    "                save.write(str(j) + ' ')\n",
    "            save.write(str(ind.fitness) + ' ' + str(ind.score) + ' ' + str(ind.dis) + '\\n')\n",
    "        save.close()\n",
    "    \n",
    "    def loadFileToPop(self, save_file, gen):\n",
    "        self.gen = gen\n",
    "        self.pop = []\n",
    "        save = open(save_file, 'r')\n",
    "        lines = save.readlines()\n",
    "        for line in lines:\n",
    "            tmp = tuple(map(float, line.split(\" \")[:]))\n",
    "            actions = []\n",
    "            for i in range(0, len(tmp) - 3):\n",
    "                actions.append(int(tmp[i]))\n",
    "            fitness = tmp[-3]\n",
    "            score = tmp[-2]\n",
    "            dis = tmp[-1]\n",
    "            self.pop.append(Individual(actions, fitness, score, dis))\n",
    "        save.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RRH-KBIODAuX",
   "metadata": {
    "id": "RRH-KBIODAuX"
   },
   "source": [
    "#Enviroment Wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "241beefa",
   "metadata": {
    "id": "241beefa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 240 * 256 * 3:\n",
    "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "def make_env(env):\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return JoypadSpace(env, SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NNCIDEBcDGVI",
   "metadata": {
    "id": "NNCIDEBcDGVI"
   },
   "source": [
    "#DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dXcqzFQyTA4T",
   "metadata": {
    "id": "dXcqzFQyTA4T"
   },
   "outputs": [],
   "source": [
    "class DQNSolver(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQNSolver, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "_QN6AXAoTECW",
   "metadata": {
    "id": "_QN6AXAoTECW"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr,\n",
    "                 dropout, exploration_max, exploration_min, exploration_decay, double_dq, pretrained):\n",
    "\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.double_dq = double_dq\n",
    "        self.pretrained = pretrained\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        if self.double_dq:  \n",
    "            self.local_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "            self.target_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "            \n",
    "            if self.pretrained:\n",
    "                self.local_net.load_state_dict(torch.load(mario_rl + \"dq1.pt\", map_location=torch.device(self.device)))\n",
    "                self.target_net.load_state_dict(torch.load(mario_rl + \"dq2.pt\", map_location=torch.device(self.device)))\n",
    "                    \n",
    "            self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
    "            self.copy = 5000  # Copy the local model weights into the target network every 5000 steps\n",
    "            self.step = 0\n",
    "        else:  \n",
    "            self.dqn = DQNSolver(state_space, action_space).to(self.device)\n",
    "            \n",
    "            if self.pretrained:\n",
    "                self.dqn.load_state_dict(torch.load(mario_rl + \"dq.pt\", map_location=torch.device(self.device)))\n",
    "            self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "        # Create memory\n",
    "        self.max_memory_size = max_memory_size\n",
    "        if self.pretrained:\n",
    "            self.STATE_MEM = torch.load(mario_rl + \"STATE_MEM.pt\")\n",
    "            self.ACTION_MEM = torch.load(mario_rl + \"ACTION_MEM.pt\")\n",
    "            self.REWARD_MEM = torch.load(mario_rl + \"REWARD_MEM.pt\")\n",
    "            self.STATE2_MEM = torch.load(mario_rl + \"STATE2_MEM.pt\")\n",
    "            self.DONE_MEM = torch.load(mario_rl + \"DONE_MEM.pt\")\n",
    "            with open(mario_rl + \"ending_position.pkl\", 'rb') as f:\n",
    "                self.ending_position = pickle.load(f)\n",
    "            with open(mario_rl + \"num_in_queue.pkl\", 'rb') as f:\n",
    "                self.num_in_queue = pickle.load(f)\n",
    "        else:\n",
    "            self.STATE_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.ACTION_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.REWARD_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.STATE2_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.DONE_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.ending_position = 0\n",
    "            self.num_in_queue = 0\n",
    "        \n",
    "        self.memory_sample_size = batch_size\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.gamma = gamma\n",
    "        self.l1 = nn.SmoothL1Loss().to(self.device) # Also known as Huber loss\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = action.float()\n",
    "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = done.float()\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
    "        \n",
    "    def recall(self):\n",
    "        # Lấy ngẫu nhiên một mẫu để học\n",
    "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
    "        \n",
    "        STATE = self.STATE_MEM[idx]\n",
    "        ACTION = self.ACTION_MEM[idx]\n",
    "        REWARD = self.REWARD_MEM[idx]\n",
    "        STATE2 = self.STATE2_MEM[idx]\n",
    "        DONE = self.DONE_MEM[idx]\n",
    "        \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE\n",
    "\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy action\n",
    "        \n",
    "        if self.double_dq:\n",
    "            self.step += 1\n",
    "        if random.random() < self.exploration_rate:  \n",
    "            return torch.tensor([[random.randrange(self.action_space)]])\n",
    "        if self.double_dq:\n",
    "            # Chọn hành động tiếp theo\n",
    "            return torch.argmax(self.local_net(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "        else:\n",
    "            return torch.argmax(self.dqn(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "\n",
    "    def copy_model(self):\n",
    "        \n",
    "        # Copy tham số từ primary sang target network\n",
    "        \n",
    "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \n",
    "        if self.double_dq and self.step % self.copy == 0:\n",
    "            self.copy_model()\n",
    "\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return\n",
    "\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.recall()\n",
    "        STATE = STATE.to(self.device)\n",
    "        ACTION = ACTION.to(self.device)\n",
    "        REWARD = REWARD.to(self.device)\n",
    "        STATE2 = STATE2.to(self.device)\n",
    "        DONE = DONE.to(self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        if self.double_dq:\n",
    "            # Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n",
    "            target = REWARD + torch.mul((self.gamma * \n",
    "                                        self.target_net(STATE2).max(1).values.unsqueeze(1)), \n",
    "                                        1 - DONE)\n",
    "\n",
    "            current = self.local_net(STATE).gather(1, ACTION.long()) # Local net approximation of Q-value\n",
    "        else:\n",
    "            # Q-Learning target is Q*(S, A) <- r + γ max_a Q(S', a) \n",
    "            target = REWARD + torch.mul((self.gamma * \n",
    "                                        self.dqn(STATE2).max(1).values.unsqueeze(1)), \n",
    "                                        1 - DONE)\n",
    "                \n",
    "            current = self.dqn(STATE).gather(1, ACTION.long())\n",
    "        \n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward() # Compute gradients\n",
    "        self.optimizer.step() # Backpropagate error\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mCbSr_ujDZMg",
   "metadata": {
    "id": "mCbSr_ujDZMg"
   },
   "source": [
    "#Runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03iSLjR0TFhj",
   "metadata": {
    "id": "03iSLjR0TFhj"
   },
   "outputs": [],
   "source": [
    "def train(mario_map, pretrained, num_episodes, ga_info = None):\n",
    "    env = gym_super_mario_bros.make(mario_map)\n",
    "    env = make_env(env) \n",
    "    observation_space = env.observation_space.shape\n",
    "    action_space = env.action_space.n\n",
    "    agent = DQNAgent(state_space=observation_space,\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=0.90,\n",
    "                     lr=0.00025,\n",
    "                     dropout=0.,\n",
    "                     exploration_max=1.0,\n",
    "                     exploration_min=0.02,\n",
    "                     exploration_decay=0.99,\n",
    "                     double_dq=True,\n",
    "                     pretrained=pretrained)\n",
    "    if ga_info != None:\n",
    "        id_ind = ga_info[0]\n",
    "        id_pop = ga_info[1]\n",
    "        ga_directory = ga_info[2]\n",
    "        listGaFile = os.listdir(ga_directory)\n",
    "        pops = []\n",
    "        for file in listGaFile:\n",
    "            g = \"\"\n",
    "            for ch in file:\n",
    "                if ch != '.':\n",
    "                    g = g + ch\n",
    "                else:\n",
    "                    break\n",
    "            g = int(g)\n",
    "            try:\n",
    "                pops.append(Population(ga_directory + '/' + file, g))\n",
    "            except:\n",
    "                pass\n",
    "        pops.sort(key=lambda x: x.gen)\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    total_scores = []\n",
    "    total_diss = []\n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        state = torch.Tensor([state])\n",
    "        total_reward = 0\n",
    "        total_score = 0\n",
    "        total_dis = 0\n",
    "        steps = 0\n",
    "        current_score = 0\n",
    "        current_stt = 0\n",
    "        idd = 0\n",
    "        if ga_info != None and random.random() < 0.25 and id_pop < len(pops):\n",
    "            print(id_pop, id_ind)\n",
    "            if id_ind == 0:\n",
    "                id_ind = int((pop_size - 1) / 2)\n",
    "                id_pop += 1\n",
    "            else:\n",
    "                id_ind -= 1\n",
    "            k  = True\n",
    "        else:\n",
    "            k = False\n",
    "        while True:\n",
    "            if k:\n",
    "                if (idd != len(pops[id_pop].pop[id_ind].actions) ):\n",
    "                    action = torch.tensor([pops[id_pop].pop[id_ind].actions[idd]])\n",
    "                    idd += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                action = agent.act(state)\n",
    "            steps += 1\n",
    "            state_next, reward, terminal, info = env.step(int(action[0]))\n",
    "            reward = reward + (info['score'] - current_score) * 5\n",
    "            current_stt = status[info['status']]\n",
    "            current_score = info['score']\n",
    "            total_reward += reward\n",
    "            total_score = info['score']\n",
    "            total_dis = info['x_pos']\n",
    "            state_next = torch.Tensor([state_next])\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)\n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            agent.remember(state, action, reward, state_next, terminal)\n",
    "            agent.experience_replay()\n",
    "            state = state_next\n",
    "            if  terminal:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        total_scores.append(total_score)\n",
    "        total_diss.append(total_dis)\n",
    "\n",
    "        print(k, \"After episode {} is {}\".format(ep_num + 1, total_rewards[-1]), total_score, total_dis)\n",
    "        num_episodes += 1    \n",
    "\n",
    "    with open(mario_rl + \"ending_position.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.ending_position, f)\n",
    "    with open(mario_rl + \"num_in_queue.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.num_in_queue, f)\n",
    "    with open(mario_rl + \"total_rewards.pkl\", \"wb\") as f:\n",
    "        pickle.dump(total_rewards, f)\n",
    "    if agent.double_dq:\n",
    "        torch.save(agent.local_net.state_dict(), mario_rl + \"dq1.pt\")\n",
    "        torch.save(agent.target_net.state_dict(), mario_rl + \"dq2.pt\")\n",
    "    else:\n",
    "        torch.save(agent.dqn.state_dict(), mario_rl + \"dq.pt\")  \n",
    "    torch.save(agent.STATE_MEM, mario_rl +  \"STATE_MEM.pt\")\n",
    "    torch.save(agent.ACTION_MEM, mario_rl + \"ACTION_MEM.pt\")\n",
    "    torch.save(agent.REWARD_MEM, mario_rl + \"REWARD_MEM.pt\")\n",
    "    torch.save(agent.STATE2_MEM, mario_rl + \"STATE2_MEM.pt\")\n",
    "    torch.save(agent.DONE_MEM,  mario_rl + \"DONE_MEM.pt\")\n",
    "    if pretrained:\n",
    "        file_reward = open(mario_rl + \"reward_episode.txt\", 'a')\n",
    "        file_score = open(mario_rl + \"score_episode.txt\", 'a')\n",
    "        file_dis = open(mario_rl + \"dis_episode.txt\", 'a')\n",
    "    else:\n",
    "        file_reward = open(mario_rl + \"reward_episode.txt\", 'w')\n",
    "        file_score = open(mario_rl + \"score_episode.txt\", 'w')\n",
    "        file_dis = open(mario_rl + \"dis_episode.txt\", 'w')\n",
    "    for i in range(len(total_rewards)):\n",
    "        file_reward.write(str(total_rewards[i]) + ' ')\n",
    "        file_score.write(str(total_scores[i]) + ' ')\n",
    "        file_dis.write(str(total_diss[i]) + ' ')\n",
    "    file_reward.close()\n",
    "    file_score.close()\n",
    "    file_dis.close()\n",
    "    env.close()\n",
    "    \n",
    "    if num_episodes > 500:\n",
    "        plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "        plt.plot([0 for _ in range(500)] + \n",
    "                 np.convolve(total_rewards, np.ones((500,))/500, mode=\"valid\").tolist())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4Z23VK7Jbx4v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Z23VK7Jbx4v",
    "outputId": "905415b2-9d96-42ab-bc5c-e4d05df08a2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n",
      " 10%|████████▎                                                                          | 1/10 [00:08<01:20,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False After episode 1 is 3093.0 500 674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [00:12<00:45,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False After episode 2 is 230.0 0 295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [01:50<05:34, 47.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False After episode 3 is 4139.0 600 1414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [01:59<04:39, 39.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23720/3670230803.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mga_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mid_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_pop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mga_database\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmario_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmario_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mga_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mga_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23720/411618063.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(mario_map, pretrained, num_episodes, ga_info)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_next\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m  \u001b[0mterminal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23720/2821466586.py\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Backpropagate error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mario_rl = 'RL_save_model/' # thư mục lưu model RL\n",
    "\n",
    "# thông số cho GA_data base\n",
    "\n",
    "id_ind = 24 # cá thể bắt đầu dạy RL\n",
    "id_pop = 75 # quần thể bắt đầu\n",
    "name_map = '1-1'\n",
    "mario_map = 'SuperMarioBros-' + name_map + '-v0' #Chọn map train\n",
    "ga_database = 'GA_database/mario_ga' + name_map #chọn chỗ database GA\n",
    "\n",
    "ga_info = [id_ind, id_pop, ga_database]\n",
    "random.seed(0)\n",
    "train(mario_map=mario_map, pretrained=True, num_episodes=10, ga_info=ga_info)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MARIO_RL.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
